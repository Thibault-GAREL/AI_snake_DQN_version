# ğŸ Snake AI with Deep Q-learning

![Python](https://img.shields.io/badge/python-3.9%2B-blue.svg)
![PyTorch](https://img.shields.io/badge/Pytorch-2.7.1%2Bcu118-red.svg)
![Numpy](https://img.shields.io/badge/Numpy-2.2.6-red.svg)
![Pygame](https://img.shields.io/badge/Pygame-2.6.1-red.svg)
![OpenPyxl](https://img.shields.io/badge/OpenPyxl-3.1.5-red.svg)  

![License](https://img.shields.io/badge/license-MIT-green.svg)  
![Contributions](https://img.shields.io/badge/contributions-welcome-orange.svg)  

## ğŸ“ Project Description 
This project showcases an AI that learns to play [my snake game](https://github.com/Thibault-GAREL/snake_game) using the deep Q-learning algorithm. No hardcoded strategy â€” the agent improves by trial, error, and reward-based learning. ğŸ§ ğŸ“ˆ

Attention ! ğŸš§ **This project is still in progress** ğŸš§  
ğŸ›ï¸ **Hyperparameters** (learning rate, epsilon decay, reward shaping) have a **huge impact** on learning performance.  
- **Problem** ğŸ˜µâ€ğŸ’« : I can find good hyperparameter ! Maybe, it is something else ğŸ˜¥.

---

## ğŸš€ Features
  ğŸ¤– Uses Deep Q-Learning with experience replay and epsilon-greedy exploration

  ğŸ§± Neural network approximates Q-values for discrete actions (e.g., accelerate, turn left/right)

<!-- 
## Example Outputs
Here is an image of what it looks like :

![Image_snake](Images/Img.png)


### ğŸ“ Notes & Observations
â³ Training is unstable at first â€” the car often spins out or crashes quickly â€” but over time, it learns to stabilize, turn properly, and sometimes follow simple roads or avoid walls.

ğŸ›ï¸ **Hyperparameters** (learning rate, epsilon decay, reward shaping) have a **huge impact** on learning performance.

Here, we can see that over 100 steps, the best path have been found (in just more than 5 min).

It is more **hesitant** for the borrowed path but **adapts better** to different circuits than **Genetic algorithm** such as [AI_driving_genetic_version](https://github.com/Thibault-GAREL/AI_driving_genetic_version) !
-->
---

## âš™ï¸ How it works
ğŸ® The AI controls the snake in a Pygame environment with basic physics and obstacles.

ğŸ§  It uses a Deep Q-Network to estimate the best action to take from any given state.

ğŸ§¾ Inputs include the distance to the border, food, and tail in all 8 cardinal directions.

ğŸ¯ Rewards are given based on food eaten and are also negative when the snake hits a wall.
---

## ğŸ“‚ Repository structure  
```bash
â”œâ”€â”€ Images/                     # Images for the README
â”‚
â”œâ”€â”€ models1/                    
â”‚   â””â”€â”€ snake_dqn_model.pth     # Saved model checkpoint
â”œâ”€â”€ models2/                    
â”œâ”€â”€ models3/                    
â”‚
â”œâ”€â”€ compteur.py                 # Counter script
â”œâ”€â”€ compteur_executions.txt     # Execution log for the counter
â”œâ”€â”€ donnees1.xlsx               # Visualization the score for the training
â”œâ”€â”€ donnees2.xlsx
â”‚
â”œâ”€â”€ exw.py                      # Excel writer script
â”œâ”€â”€ ia.py                       # AI logic
â”œâ”€â”€ main.py                     # Project entry point
â”œâ”€â”€ snake.py                    # Snake game implementation
â”‚
â”œâ”€â”€ LICENSE                     # Project license
â”œâ”€â”€ README.md                   # Main documentation
```

---

## ğŸ’» Run it on Your PC  
Clone the repository and install dependencies:  
```bash
git clone https://github.com/Thibault-GAREL/AI_snake_DQN_version.git
cd AI_snake_DQN_version

python -m venv .venv #if you don't have a virtual environnement
source .venv/bin/activate   # Linux / macOS
.venv\Scripts\activate      # Windows

pip install neat-python numpy pygame openpyxl progressbar2

python main.py
```
---

## ğŸ“– Inspiration / Sources  
I code it without any help ğŸ˜† !

Code created by me ğŸ˜, Thibault GAREL - [Github](https://github.com/Thibault-GAREL)




